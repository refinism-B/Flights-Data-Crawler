{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cf90c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import date, datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69283418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col():\n",
    "    return [\n",
    "        'query_date',\n",
    "        'flight_no',\n",
    "        'flight_type',\n",
    "        'departure_airport',\n",
    "        'departure_airport_code_1',\n",
    "        'departure_airport_code_2',\n",
    "        'arrival_airport',\n",
    "        'arrival_airport_code_1',\n",
    "        'arrival_airport_code_2',\n",
    "        'link',\n",
    "        'sync'\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_soup(corp, start_page, ss):\n",
    "    url = f'https://www.flightaware.com/live/fleet/{corp}?;offset={start_page};order=ident;sort=ASC'\n",
    "    headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36'}\n",
    "    res = ss.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "    return soup\n",
    "\n",
    "\n",
    "def page_exist_or_not(soup):\n",
    "    page_exist = True\n",
    "    for tag in soup.find_all('i'):\n",
    "        if tag.text == \"Sorry. No matching flights found; try again later.\":\n",
    "            page_exist = False\n",
    "\n",
    "    return page_exist\n",
    "\n",
    "\n",
    "def split_airport_code(code):\n",
    "    \"\"\"若機場代碼有兩種形式，會將兩者分開，回傳兩個代碼\"\"\"\n",
    "    if code is not None:\n",
    "        code = code.replace(\"(\", \"\").replace(\")\", \"\")\n",
    "        if \"/\" in code:\n",
    "            code1, code2 = code.split(\"/\")\n",
    "            code1 = code1.strip()\n",
    "            code2 = code2.strip()\n",
    "        else:\n",
    "            code1 = code\n",
    "            code2 = code\n",
    "    else:\n",
    "        code1 = None\n",
    "        code2 = None\n",
    "\n",
    "    return code1, code2\n",
    "\n",
    "\n",
    "def safe_extract(func):\n",
    "    \"\"\"判斷一個soup物件是否存在/有值，若沒有則回傳None\"\"\"\n",
    "    try:\n",
    "        return func()\n",
    "    except (IndexError, AttributeError):\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_flight_info(table_list, logtime):\n",
    "    page_data = []\n",
    "    for i in table_list:\n",
    "        single = []\n",
    "        flight_no = i('td')[0].span.a.text\n",
    "        print(f'查詢{flight_no}班機資料...')\n",
    "\n",
    "        # 紀錄日期\n",
    "        single.append(logtime)\n",
    "\n",
    "        # 班機編號\n",
    "        single.append(safe_extract(lambda: i('td')[0].span.a.text))\n",
    "\n",
    "        # 機型\n",
    "        single.append(safe_extract(lambda: i('td')[1].span.a.text))\n",
    "\n",
    "        # 起飛機場\n",
    "        single.append(safe_extract(lambda: i('td')[2]('span', dir='ltr')[0].text))\n",
    "\n",
    "        # 起飛機場代號（如有兩種則分開儲存，只有一種則重複儲存）\n",
    "        code_d = safe_extract(lambda: i('td')[2]('span', dir='ltr')[1].text)\n",
    "        code_d1, code_d2 = split_airport_code(code_d)\n",
    "        single.append(code_d1)\n",
    "        single.append(code_d2)\n",
    "\n",
    "        # 降落機場\n",
    "        single.append(safe_extract(lambda: i('td')[3]('span', dir='ltr')[0].text))\n",
    "\n",
    "        # 降落機場代號（如有兩種則分開儲存，只有一種則重複儲存）\n",
    "        code_a = safe_extract(lambda: i('td')[3]('span', dir='ltr')[1].text)\n",
    "        code_a1, code_a2 = split_airport_code(code_a)\n",
    "        single.append(code_a1)\n",
    "        single.append(code_a2)\n",
    "\n",
    "        # 連結\n",
    "        single.append('https://www.flightaware.com' + safe_extract(lambda: i('td')[0].span.a['href']))\n",
    "\n",
    "        # 同步標記\n",
    "        single.append(0)\n",
    "\n",
    "        # 存回page_data list\n",
    "        page_data.append(single)\n",
    "\n",
    "    return page_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35935269",
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_corp = [\"EVA\", \"CAL\", \"SJX\", \"TTW\"]\n",
    "\n",
    "for corp in flight_corp:\n",
    "    # 判斷總列表.csv檔是否存在，若不存在則先建立一個只有columns的空表格\n",
    "    folder = r\"C:\\Users\\add41\\Documents\\Data_Engineer\\Project\\Flights-Data-Crawler\\Data\"\n",
    "    file = f\"{corp}_FlightList.csv\"\n",
    "    file_path = os.path.join(folder, file)\n",
    "    path = Path(file_path)\n",
    "\n",
    "    if path.exists():\n",
    "        df_main = pd.read_csv(file_path)\n",
    "    else:\n",
    "        columns = get_col()\n",
    "        df_main = pd.DataFrame(columns=columns)\n",
    "\n",
    "\n",
    "    # 建立空list（為建立dataframe預備）並設定起始頁數，建立ss連線\n",
    "    data = []\n",
    "    logtime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    start_page = 0\n",
    "    ss = requests.Session()\n",
    "\n",
    "    while True:\n",
    "        # 網頁為20筆一頁，設定從第0筆開始查詢，每次回圈+20，直到查無資料後break\n",
    "        # 利用ss.get發出請求並轉換出soup物件\n",
    "        soup = get_soup(corp, start_page, ss)\n",
    "        print(f'開始查詢{corp}的第{start_page}到{start_page + 20}筆資料...')\n",
    "\n",
    "        # 若仍有資料 found = False 則繼續迴圈，分別尋找兩個標籤（兩種皆有連結）並合併list\n",
    "        if page_exist_or_not(soup):\n",
    "            table_list = soup('table', class_='prettyTable fullWidth')[0]('tr')[2:]\n",
    "            page_data = get_flight_info(table_list, logtime)\n",
    "\n",
    "            for single_data in page_data:\n",
    "                data.append(single_data)\n",
    "\n",
    "            # 完成後查詢筆數+20並稍微等待後在進行下一次迴圈\n",
    "            print(f'完成存取{corp}的第{start_page}到{start_page + 20}筆資料')\n",
    "            start_page += 20\n",
    "            time.sleep(10)\n",
    "\n",
    "        # 當查無資料時 found = True 顯示查無資料並終止迴圈\n",
    "        else:\n",
    "            print(f'{corp}沒有第{start_page}到{start_page + 20}筆資料')\n",
    "            break\n",
    "\n",
    "    print(f'已完成{corp}存取資料')\n",
    "\n",
    "\n",
    "    # 根據爬蟲資料建立Dataframe\n",
    "    columns = get_col()\n",
    "\n",
    "    df_corp = pd.DataFrame(columns=columns, data=data)\n",
    "    print(f'{corp}新資料建檔完成')\n",
    "\n",
    "    # 直接將新資料與舊資料合併\n",
    "    df_main = pd.concat([df_main, df_corp], ignore_index=True)\n",
    "\n",
    "    # 對合併後的資料使用drop_duplicates，將重複值刪去，並覆蓋回df_main\n",
    "    df_main = df_main.drop_duplicates(subset='link', keep = 'first').reset_index(drop=True)\n",
    "\n",
    "    # 將query_date欄位轉換為datetime物件\n",
    "    df_main['query_date'] = pd.to_datetime(df_main['query_date'])\n",
    "\n",
    "    # 將新的df_main進行存檔\n",
    "    df_main.to_csv(file_path, index=False)\n",
    "\n",
    "    print(f'完成{corp}資料更新，目前資料筆數：{len(df_main)}')\n",
    "    print('5秒後繼續...')\n",
    "    time.sleep(5)\n",
    "\n",
    "print('已完成所有航空公司資料更新！')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flights-data-crawler-HkKwTBFH-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
