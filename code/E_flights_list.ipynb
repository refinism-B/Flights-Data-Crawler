{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cf90c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import date, datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69283418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col():\n",
    "    return [\n",
    "        'query_date',\n",
    "        'flight_no',\n",
    "        'flight_type',\n",
    "        'departure_airport',\n",
    "        'departure_airport_code_1',\n",
    "        'departure_airport_code_2',\n",
    "        'arrival_airport',\n",
    "        'arrival_airport_code_1',\n",
    "        'arrival_airport_code_2',\n",
    "        'link',\n",
    "        # 'sync'\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_soup(corp, start_page, ss):\n",
    "    url = f'https://www.flightaware.com/live/fleet/{corp}?;offset={start_page};order=ident;sort=ASC'\n",
    "    headers = {\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36'}\n",
    "    res = ss.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "    return soup\n",
    "\n",
    "\n",
    "def page_exist_or_not(soup):\n",
    "    page_exist = True\n",
    "    for tag in soup.find_all('i'):\n",
    "        if tag.text == \"Sorry. No matching flights found; try again later.\":\n",
    "            page_exist = False\n",
    "\n",
    "    return page_exist\n",
    "\n",
    "\n",
    "def split_airport_code(code):\n",
    "    \"\"\"若機場代碼有兩種形式，會將兩者分開，回傳兩個代碼\"\"\"\n",
    "    if code is not None:\n",
    "        code = code.replace(\"(\", \"\").replace(\")\", \"\")\n",
    "        if \"/\" in code:\n",
    "            code1, code2 = code.split(\"/\")\n",
    "            code1 = code1.strip()\n",
    "            code2 = code2.strip()\n",
    "        else:\n",
    "            code1 = code\n",
    "            code2 = code\n",
    "    else:\n",
    "        code1 = None\n",
    "        code2 = None\n",
    "\n",
    "    return code1, code2\n",
    "\n",
    "\n",
    "def safe_extract(func):\n",
    "    \"\"\"判斷一個soup物件是否存在/有值，若沒有則回傳None\"\"\"\n",
    "    try:\n",
    "        return func()\n",
    "    except (IndexError, AttributeError):\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_flight_info(table_list, logtime):\n",
    "    page_data = []\n",
    "    for i in table_list:\n",
    "        single = []\n",
    "        flight_no = i('td')[0].span.a.text\n",
    "        print(f'查詢{flight_no}班機資料...')\n",
    "\n",
    "        # 紀錄日期\n",
    "        single.append(logtime)\n",
    "\n",
    "        # 班機編號\n",
    "        single.append(safe_extract(lambda: i('td')[0].span.a.text))\n",
    "\n",
    "        # 機型\n",
    "        single.append(safe_extract(lambda: i('td')[1].span.a.text))\n",
    "\n",
    "        # 起飛機場\n",
    "        single.append(safe_extract(lambda: i('td')[\n",
    "                      2]('span', dir='ltr')[0].text))\n",
    "\n",
    "        # 起飛機場代號（如有兩種則分開儲存，只有一種則重複儲存）\n",
    "        code_d = safe_extract(lambda: i('td')[2]('span', dir='ltr')[1].text)\n",
    "        code_d1, code_d2 = split_airport_code(code_d)\n",
    "        single.append(code_d1)\n",
    "        single.append(code_d2)\n",
    "\n",
    "        # 降落機場\n",
    "        single.append(safe_extract(lambda: i('td')[\n",
    "                      3]('span', dir='ltr')[0].text))\n",
    "\n",
    "        # 降落機場代號（如有兩種則分開儲存，只有一種則重複儲存）\n",
    "        code_a = safe_extract(lambda: i('td')[3]('span', dir='ltr')[1].text)\n",
    "        code_a1, code_a2 = split_airport_code(code_a)\n",
    "        single.append(code_a1)\n",
    "        single.append(code_a2)\n",
    "\n",
    "        # 連結\n",
    "        single.append('https://www.flightaware.com' +\n",
    "                      safe_extract(lambda: i('td')[0].span.a['href']))\n",
    "\n",
    "        # # 同步標記\n",
    "        # single.append(0)\n",
    "\n",
    "        # 存回page_data list\n",
    "        page_data.append(single)\n",
    "\n",
    "    return page_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35935269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "開始查詢EVA的第0到20筆資料...\n",
      "查詢EVA10班機資料...\n",
      "查詢EVA108班機資料...\n",
      "查詢EVA132班機資料...\n",
      "查詢EVA16班機資料...\n",
      "查詢EVA166班機資料...\n",
      "查詢EVA170班機資料...\n",
      "查詢EVA182班機資料...\n",
      "查詢EVA184班機資料...\n",
      "查詢EVA192班機資料...\n",
      "查詢EVA198班機資料...\n",
      "查詢EVA211班機資料...\n",
      "查詢EVA225班機資料...\n",
      "查詢EVA233班機資料...\n",
      "查詢EVA237班機資料...\n",
      "查詢EVA257班機資料...\n",
      "查詢EVA26班機資料...\n",
      "查詢EVA265班機資料...\n",
      "查詢EVA28班機資料...\n",
      "查詢EVA281班機資料...\n",
      "查詢EVA32班機資料...\n",
      "完成存取EVA的第0到20筆資料\n",
      "開始查詢EVA的第20到40筆資料...\n",
      "查詢EVA395班機資料...\n",
      "查詢EVA5班機資料...\n",
      "查詢EVA52班機資料...\n",
      "查詢EVA6061班機資料...\n",
      "查詢EVA62班機資料...\n",
      "查詢EVA620班機資料...\n",
      "查詢EVA621班機資料...\n",
      "查詢EVA637班機資料...\n",
      "查詢EVA65班機資料...\n",
      "查詢EVA651班機資料...\n",
      "查詢EVA658班機資料...\n",
      "查詢EVA668班機資料...\n",
      "查詢EVA67班機資料...\n",
      "查詢EVA68班機資料...\n",
      "查詢EVA7班機資料...\n",
      "查詢EVA707班機資料...\n",
      "查詢EVA71班機資料...\n",
      "查詢EVA829班機資料...\n",
      "查詢EVA845班機資料...\n",
      "查詢EVA851班機資料...\n",
      "完成存取EVA的第20到40筆資料\n",
      "開始查詢EVA的第40到60筆資料...\n",
      "查詢EVA87班機資料...\n",
      "查詢EVA8802班機資料...\n",
      "完成存取EVA的第40到60筆資料\n",
      "開始查詢EVA的第60到80筆資料...\n",
      "EVA沒有第60到80筆資料\n",
      "已完成EVA存取資料\n",
      "EVA新資料建檔完成\n",
      "完成EVA資料更新，目前資料筆數：42\n",
      "5秒後繼續...\n",
      "開始查詢CAL的第0到20筆資料...\n",
      "查詢CAL102班機資料...\n",
      "查詢CAL107班機資料...\n",
      "查詢CAL120班機資料...\n",
      "查詢CAL130班機資料...\n",
      "查詢CAL151班機資料...\n",
      "查詢CAL154班機資料...\n",
      "查詢CAL156班機資料...\n",
      "查詢CAL160班機資料...\n",
      "查詢CAL164班機資料...\n",
      "查詢CAL166班機資料...\n",
      "查詢CAL1857班機資料...\n",
      "查詢CAL188班機資料...\n",
      "查詢CAL198班機資料...\n",
      "查詢CAL22班機資料...\n",
      "查詢CAL220班機資料...\n",
      "查詢CAL223班機資料...\n",
      "查詢CAL260班機資料...\n",
      "查詢CAL278班機資料...\n",
      "查詢CAL32班機資料...\n",
      "查詢CAL4班機資料...\n",
      "完成存取CAL的第0到20筆資料\n",
      "開始查詢CAL的第20到40筆資料...\n",
      "查詢CAL5班機資料...\n",
      "查詢CAL501班機資料...\n",
      "查詢CAL511班機資料...\n",
      "查詢CAL5122班機資料...\n",
      "查詢CAL5137班機資料...\n",
      "查詢CAL5141班機資料...\n",
      "查詢CAL5233班機資料...\n",
      "查詢CAL5234班機資料...\n",
      "查詢CAL5239班機資料...\n",
      "查詢CAL5267班機資料...\n",
      "查詢CAL5311班機資料...\n",
      "查詢CAL5346班機資料...\n",
      "查詢CAL5522班機資料...\n",
      "查詢CAL5637班機資料...\n",
      "查詢CAL61班機資料...\n",
      "查詢CAL63班機資料...\n",
      "查詢CAL701班機資料...\n",
      "查詢CAL705班機資料...\n",
      "查詢CAL721班機資料...\n",
      "查詢CAL73班機資料...\n",
      "完成存取CAL的第20到40筆資料\n",
      "開始查詢CAL的第40到60筆資料...\n",
      "查詢CAL75班機資料...\n",
      "查詢CAL753班機資料...\n",
      "查詢CAL781班機資料...\n",
      "查詢CAL791班機資料...\n",
      "查詢CAL8班機資料...\n",
      "查詢CAL833班機資料...\n",
      "查詢CAL903班機資料...\n",
      "查詢CAL991班機資料...\n",
      "完成存取CAL的第40到60筆資料\n",
      "開始查詢CAL的第60到80筆資料...\n",
      "CAL沒有第60到80筆資料\n",
      "已完成CAL存取資料\n",
      "CAL新資料建檔完成\n",
      "完成CAL資料更新，目前資料筆數：48\n",
      "5秒後繼續...\n",
      "開始查詢SJX的第0到20筆資料...\n",
      "查詢SJX12班機資料...\n",
      "查詢SJX2班機資料...\n",
      "查詢SJX201班機資料...\n",
      "查詢SJX233班機資料...\n",
      "查詢SJX300班機資料...\n",
      "查詢SJX306班機資料...\n",
      "查詢SJX711班機資料...\n",
      "查詢SJX715班機資料...\n",
      "查詢SJX771班機資料...\n",
      "查詢SJX781班機資料...\n",
      "查詢SJX800班機資料...\n",
      "查詢SJX820班機資料...\n",
      "完成存取SJX的第0到20筆資料\n",
      "開始查詢SJX的第20到40筆資料...\n",
      "SJX沒有第20到40筆資料\n",
      "已完成SJX存取資料\n",
      "SJX新資料建檔完成\n",
      "完成SJX資料更新，目前資料筆數：12\n",
      "5秒後繼續...\n",
      "開始查詢TTW的第0到20筆資料...\n",
      "查詢TTW200班機資料...\n",
      "查詢TTW210班機資料...\n",
      "查詢TTW231班機資料...\n",
      "查詢TTW234班機資料...\n",
      "查詢TTW236班機資料...\n",
      "查詢TTW280班機資料...\n",
      "查詢TTW284班機資料...\n",
      "查詢TTW654班機資料...\n",
      "查詢TTW724班機資料...\n",
      "完成存取TTW的第0到20筆資料\n",
      "開始查詢TTW的第20到40筆資料...\n",
      "TTW沒有第20到40筆資料\n",
      "已完成TTW存取資料\n",
      "TTW新資料建檔完成\n",
      "完成TTW資料更新，目前資料筆數：9\n",
      "5秒後繼續...\n",
      "已完成所有航空公司資料更新！\n"
     ]
    }
   ],
   "source": [
    "flight_corp = [\"EVA\", \"CAL\", \"SJX\", \"TTW\"]\n",
    "today = date.today().strftime(\"%Y%m%d\")\n",
    "\n",
    "for corp in flight_corp:\n",
    "    # 判斷總列表.csv檔是否存在，若不存在則先建立一個只有columns的空表格\n",
    "    folder = r\"C:\\Users\\add41\\Documents\\Data_Engineer\\Project\\Flights-Data-Crawler\\Data\"\n",
    "    file = f\"{today}_{corp}_FlightList.csv\"\n",
    "    file_path = os.path.join(folder, file)\n",
    "    path = Path(file_path)\n",
    "\n",
    "    if path.exists():\n",
    "        df_main = pd.read_csv(file_path)\n",
    "    else:\n",
    "        columns = get_col()\n",
    "        df_main = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # 建立空list（為建立dataframe預備）並設定起始頁數，建立ss連線\n",
    "    data = []\n",
    "    logtime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    start_page = 0\n",
    "    ss = requests.Session()\n",
    "\n",
    "    while True:\n",
    "        # 網頁為20筆一頁，設定從第0筆開始查詢，每次回圈+20，直到查無資料後break\n",
    "        # 利用ss.get發出請求並轉換出soup物件\n",
    "        soup = get_soup(corp, start_page, ss)\n",
    "        print(f'開始查詢{corp}的第{start_page}到{start_page + 20}筆資料...')\n",
    "\n",
    "        # 若仍有資料 found = False 則繼續迴圈，分別尋找兩個標籤（兩種皆有連結）並合併list\n",
    "        if page_exist_or_not(soup):\n",
    "            table_list = soup('table', class_='prettyTable fullWidth')[\n",
    "                0]('tr')[2:]\n",
    "            page_data = get_flight_info(table_list, logtime)\n",
    "\n",
    "            for single_data in page_data:\n",
    "                data.append(single_data)\n",
    "\n",
    "            # 完成後查詢筆數+20並稍微等待後在進行下一次迴圈\n",
    "            print(f'完成存取{corp}的第{start_page}到{start_page + 20}筆資料')\n",
    "            start_page += 20\n",
    "            time.sleep(10)\n",
    "\n",
    "        # 當查無資料時 found = True 顯示查無資料並終止迴圈\n",
    "        else:\n",
    "            print(f'{corp}沒有第{start_page}到{start_page + 20}筆資料')\n",
    "            break\n",
    "\n",
    "    print(f'已完成{corp}存取資料')\n",
    "\n",
    "    # 根據爬蟲資料建立Dataframe\n",
    "    columns = get_col()\n",
    "\n",
    "    df_corp = pd.DataFrame(columns=columns, data=data)\n",
    "    print(f'{corp}新資料建檔完成')\n",
    "\n",
    "    # 直接將新資料與舊資料合併\n",
    "    df_main = pd.concat([df_main, df_corp], ignore_index=True)\n",
    "\n",
    "    # 對合併後的資料使用drop_duplicates，將重複值刪去，並覆蓋回df_main\n",
    "    df_main = df_main.drop_duplicates(\n",
    "        subset='link', keep='first').reset_index(drop=True)\n",
    "\n",
    "    # 將query_date欄位轉換為datetime物件\n",
    "    df_main['query_date'] = pd.to_datetime(df_main['query_date'])\n",
    "\n",
    "    # 將新的df_main進行存檔\n",
    "    df_main.to_csv(file_path, index=False)\n",
    "\n",
    "    print(f'完成{corp}資料更新，目前資料筆數：{len(df_main)}')\n",
    "    print('5秒後繼續...')\n",
    "    time.sleep(5)\n",
    "\n",
    "print('已完成所有航空公司資料更新！')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flights-data-crawler-HkKwTBFH-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
