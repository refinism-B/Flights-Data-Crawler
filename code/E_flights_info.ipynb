{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e42ef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import date, datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2f6767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_or_build(folder, file):\n",
    "    file_path = os.path.join(folder, file)\n",
    "    path = Path(file_path)\n",
    "\n",
    "    # 若檔案不存在則先新建空的df並存檔\n",
    "    if path.exists():\n",
    "        df = pd.read_csv(file_path)\n",
    "    else:\n",
    "        columns = get_col()\n",
    "        df = pd.DataFrame(columns=columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_col():\n",
    "    return [\n",
    "        'flight_NO',\n",
    "        'flight_type',\n",
    "        'flight_company',\n",
    "        'fly_distance',\n",
    "        'departure_airport_code',\n",
    "        'departure_city',\n",
    "        'arrival_airport_code',\n",
    "        'arrival_city',\n",
    "        'departure_date',\n",
    "        'leave_gate_estimate',\n",
    "        'leave_gate_actual',\n",
    "        'departure_time_estimate',\n",
    "        'departure_time_actual',\n",
    "        'departure_timezone',\n",
    "        'arrival_date',\n",
    "        'landing_time_estimate',\n",
    "        'landing_time_actual',\n",
    "        'arrive_gate_estimate',\n",
    "        'arrive_gate_actual',\n",
    "        'arrive_timezone',\n",
    "        'link'\n",
    "    ]\n",
    "\n",
    "\n",
    "def source_list_mask(df_list, df_table):\n",
    "    today = pd.Timestamp.now()\n",
    "\n",
    "    mask_1 = (df_list['sync'] == 0)\n",
    "    mask_2 = ((today - df_list['query_date']).dt.days >= 2)\n",
    "    mask_3 = df_list['link'].isin(df_table['link'])\n",
    "\n",
    "    source = df_list[mask_1 & mask_2 & ~mask_3]\n",
    "\n",
    "    return source\n",
    "\n",
    "\n",
    "def get_page_source(url, driver):\n",
    "    driver.get(url)\n",
    "\n",
    "    # 網頁內有JavaScript動態生成內容，故設定等待網頁讀取完畢後再動作\n",
    "    wait = WebDriverWait(driver, 15)\n",
    "    element = wait.until(\n",
    "        EC.presence_of_all_elements_located(\n",
    "            (By.CLASS_NAME, \"flightPageSummaryDepartureDay\"))\n",
    "    )\n",
    "\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    return page_source\n",
    "\n",
    "\n",
    "def trans_date_from_chinese(chinese_date):\n",
    "    \"\"\"將中文日期格式轉為datetime格式\"\"\"\n",
    "    clean_date = chinese_date.split(\"(\")[0].strip()\n",
    "    fmt = \"%Y年 %m月 %d日\"\n",
    "    trans = datetime.strptime(clean_date, fmt)\n",
    "\n",
    "    return trans\n",
    "\n",
    "\n",
    "def find_tag(div_list, target_str: str):\n",
    "    \"\"\"用於尋找特定字串標籤的index\"\"\"\n",
    "    target = 0\n",
    "    for i in div_list:\n",
    "        if i.get_text() == target_str:\n",
    "            break\n",
    "        else:\n",
    "            target += 1\n",
    "    return target\n",
    "\n",
    "\n",
    "def safe_extract(func):\n",
    "    \"\"\"判斷一個soup物件是否存在/有值，若沒有則回傳None\"\"\"\n",
    "    try:\n",
    "        return func()\n",
    "    except (IndexError, AttributeError):\n",
    "        return None\n",
    "\n",
    "\n",
    "def gate_exist(soup):\n",
    "    \"\"\"判斷一個航班頁面中是否有到/離閘口資料\"\"\"\n",
    "    gate = 0\n",
    "    x = soup\n",
    "    for i in x('div'):\n",
    "        if '閘口' or \"停机位\" in i.text:\n",
    "            gate = 1\n",
    "            break\n",
    "    return gate\n",
    "\n",
    "\n",
    "def split_tz(time_str: str):\n",
    "    \"\"\"將帶有時區的時間字串分割，得到[time, timezone]列表\"\"\"\n",
    "    if \"(\" in time_str:\n",
    "        time_str = time_str.split(\"(\")[0].strip()\n",
    "\n",
    "    time_, tz = time_str.split(' ')\n",
    "    return time_, tz\n",
    "\n",
    "\n",
    "def crawl_flight_data(soup, url, gate_exist: bool):\n",
    "    \"\"\"當該班機有到/離閘門資料時使用的爬蟲\"\"\"\n",
    "    flight_data = []\n",
    "\n",
    "    # 班機基本資料。較容易在各網頁中出現差異，故先使用函式取得定位，再去取得資訊\n",
    "    div_list = soup('div', class_='flightPageDataLabel')\n",
    "\n",
    "    # 航班編號、機型、航空公司、飛行距離\n",
    "    flight_data.append(safe_extract(lambda: soup(\n",
    "        'div', class_='flightPageIdent')[0].h1.text.strip()))\n",
    "    flight_data.append(safe_extract(lambda: soup('div', class_='flightPageDataRow')[\n",
    "                       0]('div', class_='flightPageData')[0].text.strip().replace('\\xa0', ' ')))\n",
    "    flight_data.append(safe_extract(lambda: soup('div', class_='flightPageDataRow')[\n",
    "                       2]('div', class_='flightPageData')[0].text.strip().split('\\n')[0]))\n",
    "    flight_data.append(safe_extract(lambda: soup('div', class_='flightPageDataRow')[\n",
    "                       5].span.text.strip().replace(',', '').replace(\"\\n\", \"\").replace(\"\\t\", \"\")))\n",
    "\n",
    "    # 起飛機場、起飛城市\n",
    "    flight_data.append(safe_extract(lambda: soup('div', class_='flightPageSummaryOrigin')[\n",
    "                       0]('span', class_='displayFlexElementContainer')[0].text.strip()))\n",
    "    flight_data.append(safe_extract(lambda: soup('div', class_='flightPageSummaryOrigin')[\n",
    "                       0]('span', class_='flightPageSummaryCity')[0].text.strip()))\n",
    "\n",
    "    # 降落機場、降落城市\n",
    "    flight_data.append(safe_extract(lambda: soup('div', class_='flightPageSummaryDestination')[\n",
    "                       0]('span', class_='displayFlexElementContainer')[0].text.strip()))\n",
    "    flight_data.append(safe_extract(lambda: soup('div', class_='flightPageSummaryDestination')[\n",
    "                       0]('span', class_='destinationCity')[0].text.strip()))\n",
    "\n",
    "    # 起飛日期\n",
    "    flight_data.append(safe_extract(lambda: soup(\n",
    "        'span', class_='flightPageSummaryDepartureDay')[0].text))\n",
    "\n",
    "    if gate_exist == 1:\n",
    "        # 預計/實際離開閘門時間\n",
    "        lg_e_time, lg_e_tz = split_tz(safe_extract(lambda: soup('div', class_='flightPageDataTimesChild')[\n",
    "                                      0].span.text.strip().replace('\\xa0', ' ').replace('\\n', '').replace('\\t', '')))\n",
    "        lg_a_time, lg_a_tz = split_tz(safe_extract(lambda: soup('div', class_='flightPageDataTimesChild')[0](\n",
    "            'div', class_=\"flightPageDataActualTimeText\")[0].text.strip().replace('\\xa0', ' ').replace('\\\\n', '').replace('\\\\t', '')))\n",
    "\n",
    "        flight_data.append(lg_e_time)\n",
    "        flight_data.append(lg_a_time)\n",
    "\n",
    "        # 預計/實際起飛時間\n",
    "        d_e_time, d_e_tz = split_tz(safe_extract(lambda: soup('div', class_='flightPageDataTimesChild')[\n",
    "                                    1]('span')[1].text.strip().replace('\\xa0', ' ').replace('\\n', '').replace('\\t', '')))\n",
    "        d_a_time, d_a_tz = split_tz(safe_extract(lambda: soup('div', class_='flightPageDataTimesChild')[\n",
    "                                    1]('span')[0].text.strip().replace('\\xa0', ' ').replace('\\n', '').replace('\\t', '')))\n",
    "\n",
    "        flight_data.append(d_e_time)\n",
    "        flight_data.append(d_a_time)\n",
    "\n",
    "    else:\n",
    "        # 預計/實際離開閘門時間\n",
    "        flight_data.append(None)\n",
    "        flight_data.append(None)\n",
    "\n",
    "        # 預計/實際起飛時間\n",
    "        d_e_time, d_e_tz = split_tz(safe_extract(lambda: soup('div', class_='flightPageDataTimesChild')[\n",
    "                                    0]('span')[1].text.strip().replace('\\xa0', ' ').replace('\\n', '').replace('\\t', '')))\n",
    "        d_a_time, d_a_tz = split_tz(safe_extract(lambda: soup('div', class_='flightPageDataTimesChild')[\n",
    "                                    0]('span')[0].text.strip().replace('\\xa0', ' ').replace('\\n', '').replace('\\t', '')))\n",
    "\n",
    "        flight_data.append(d_e_time)\n",
    "        flight_data.append(d_a_time)\n",
    "\n",
    "    # 起飛時區\n",
    "    flight_data.append(d_a_tz)\n",
    "\n",
    "    # 抵達日期\n",
    "    flight_data.append(safe_extract(lambda: soup(\n",
    "        'span', class_='flightPageSummaryArrivalDay')[0].text))\n",
    "\n",
    "    if gate_exist == 1:\n",
    "        # 預計/實際降落時間\n",
    "        a_e_time, a_e_tz = split_tz(safe_extract(lambda: soup('div', class_='flightPageDataTimesChild')[\n",
    "                                    2]('span')[1].text.strip().replace('\\xa0', ' ').replace('\\n', '').replace('\\t', '')))\n",
    "        a_a_time, a_a_tz = split_tz(safe_extract(lambda: soup('div', class_='flightPageDataTimesChild')[\n",
    "                                    2]('span')[0].text.strip().replace('\\xa0', ' ').replace('\\n', '').replace('\\t', '')))\n",
    "\n",
    "        flight_data.append(a_e_time)\n",
    "        flight_data.append(a_a_time)\n",
    "\n",
    "        # 預計/實際抵達閘門時間\n",
    "        ag_e_time, ag_e_tz = split_tz(safe_extract(lambda: soup('div', class_='flightPageDataTimesChild')[\n",
    "                                      3]('span')[1].text.strip().replace('\\xa0', ' ').replace('\\n', '').replace('\\t', '')))\n",
    "        ag_a_time, ag_a_tz = split_tz(safe_extract(lambda: soup('div', class_='flightPageDataTimesChild')[3](\n",
    "            'div', class_='flightPageDataActualTimeText')[0].span.text.strip().replace('\\xa0', ' ').replace('\\n', '').replace('\\t', '')))\n",
    "\n",
    "        flight_data.append(ag_e_time)\n",
    "        flight_data.append(ag_a_time)\n",
    "\n",
    "    else:\n",
    "        # 預計/實際降落時間\n",
    "        a_e_time, a_e_tz = split_tz(safe_extract(lambda: soup('div', class_='flightPageDataTimesChild')[\n",
    "                                    1]('span')[1].text.strip().replace('\\xa0', ' ').replace('\\n', '').replace('\\t', '')))\n",
    "        a_a_time, a_a_tz = split_tz(safe_extract(lambda: soup('div', class_='flightPageDataTimesChild')[\n",
    "                                    1]('span')[0].text.strip().replace('\\xa0', ' ').replace('\\n', '').replace('\\t', '')))\n",
    "\n",
    "        flight_data.append(a_e_time)\n",
    "        flight_data.append(a_a_time)\n",
    "\n",
    "        # 預計/實際抵達閘門時間\n",
    "        flight_data.append(None)\n",
    "        flight_data.append(None)\n",
    "\n",
    "    # 降落時區\n",
    "    flight_data.append(a_a_tz)\n",
    "\n",
    "    # 紀錄該航班網址，若有需要可再重新訪問\n",
    "    flight_data.append(url)\n",
    "\n",
    "    return flight_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a8dda82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目前尚無EVA資料可供查詢\n",
      "目前尚無CAL資料可供查詢\n",
      "目前尚無SJX資料可供查詢\n",
      "目前尚無TTW資料可供查詢\n",
      "已更新所有資料！\n"
     ]
    }
   ],
   "source": [
    "# 航空公司清單\n",
    "flight_corp = [\"EVA\", \"CAL\", \"SJX\", \"TTW\"]\n",
    "today = date.today()\n",
    "read_date = today - timedelta(days=2)\n",
    "read_date_str = read_date.strftime(\"%Y%m%d\")\n",
    "\n",
    "for corp in flight_corp:\n",
    "    # 設定航班資訊的檔案路徑\n",
    "    table_folder = r\"C:\\Users\\add41\\Documents\\Data_Engineer\\Project\\Flights-Data-Crawler\\Data\"\n",
    "    table_file = f\"{read_date_str}_{corp}_FlightsTable.csv\"\n",
    "    df_table = read_or_build(folder=table_folder, file=table_file)\n",
    "\n",
    "    # 設定航班列表的資料表路徑，並將列表資料讀入\n",
    "    list_folder = r\"C:\\Users\\add41\\Documents\\Data_Engineer\\Project\\Flights-Data-Crawler\\Data\"\n",
    "    list_file = f\"{read_date_str}_{corp}_FlightList.csv\"\n",
    "    list_path = os.path.join(list_folder, list_file)\n",
    "    path = Path(list_path)\n",
    "\n",
    "    if path.exists():\n",
    "        df_list = pd.read_csv(list_path)\n",
    "    else:\n",
    "        print(f\"目前尚無{corp}資料可供查詢\")\n",
    "        continue\n",
    "\n",
    "    # 設定篩選條件，保留符合條件的（未同步，且日期超過兩天以上）\n",
    "    # df_list['query_date'] = pd.to_datetime(df_list['query_date'])\n",
    "    # source = source_list_mask(df_list, df_table)\n",
    "    source = df_list.copy()\n",
    "\n",
    "    # 建立dataframe需要的data list\n",
    "    data = []\n",
    "\n",
    "    # 建立selenium連線\n",
    "    selenium_url = \"http://localhost:4444/wd/hub\"\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    print('建立連線')\n",
    "\n",
    "    # 根據df_list中的link欄位跑回圈，逐一進入網頁取得html編碼\n",
    "    for url in source['link']:\n",
    "        with webdriver.Remote(command_executor=selenium_url, options=options) as driver:\n",
    "            try:\n",
    "                page_source = get_page_source(url, driver)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"無法存取 {url}: {e}\")\n",
    "                continue\n",
    "\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        flight_no = soup('div', class_='flightPageIdent')[0].h1.text.strip()\n",
    "\n",
    "        # 根據取得的soup物件，開始抓取各項資訊\n",
    "        print(f'開始查詢{flight_no}班機資訊資訊...')\n",
    "\n",
    "        try:\n",
    "            gate_exist = gate_exist(soup)\n",
    "            print(f'{flight_no}航班有無閘門資訊：{gate_exist}')\n",
    "            flight_data = crawl_flight_data(\n",
    "                soup, url, gate_exist=gate_exist)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'發生錯誤：{e}')\n",
    "\n",
    "        data.append(flight_data)\n",
    "        print(f'完成存取{flight_no}航班資料')\n",
    "        time.sleep(7)\n",
    "\n",
    "    columns = get_col()\n",
    "    df_flight = pd.DataFrame(columns=columns, data=data)\n",
    "\n",
    "    # 將本次爬取航班中，航行未完成（沒有降落時間）的資料先去除，待下次再爬取\n",
    "    df_flight = df_flight.dropna(subset=['landing_time_actual'])\n",
    "\n",
    "    # 將爬取的新資料與原本的table資料合併\n",
    "    df_combine = pd.concat([df_table, df_flight], ignore_index=True)\n",
    "\n",
    "    # 根據link欄位再去除可能的重複值\n",
    "    df_combine = df_combine.drop_duplicates(subset='link', keep='first')\n",
    "\n",
    "    # 將去除重複後的資料存檔\n",
    "    table_path = os.path.join(table_folder, table_file)\n",
    "    df_combine.to_csv(table_path, index=False)\n",
    "\n",
    "    # # 將已經爬取過的航班sync欄位改為1，避免下次重複爬取\n",
    "    # mask_done = df_list['link'].isin(df_flight['link'])\n",
    "    # df_list.loc[mask_done, 'sync'] = 1\n",
    "\n",
    "    # # 將修改後的df_list再存檔回FlightList.csv檔案\n",
    "    # list_path = os.path.join(list_folder, list_file)\n",
    "    # df_list.to_csv(list_path, index=False)\n",
    "\n",
    "print('已更新所有資料！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca167f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flights-data-crawler-HkKwTBFH-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
